INFO - 04/12/22 01:28:56 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f6ee8066a00>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:28:56 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (audio_enc): AudioEncoder(
                                           (conv_layers): ModuleList(
                                             (0): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (1): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (2): AdaptiveAvgPool1d(output_size=200)
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_ppro): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_ppro): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (CLS_v): Linear(in_features=768, out_features=768, bias=True)
                                         (CLS_a): Linear(in_features=768, out_features=768, bias=True)
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=201, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=201, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=201, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=201, bias=True)
                                         (transfm_v2a): Linear(in_features=201, out_features=201, bias=True)
                                         (transfm_a2v): Linear(in_features=201, out_features=201, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:28:57 - 0:00:01 - Training..
INFO - 04/12/22 01:37:49 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7fbb7cf26ee0>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:37:49 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:37:50 - 0:00:01 - Training..
INFO - 04/12/22 01:40:16 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f8453a22f10>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:40:16 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:40:17 - 0:00:01 - Training..
INFO - 04/12/22 01:41:59 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f2294694e80>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:41:59 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:42:00 - 0:00:01 - Training..
INFO - 04/12/22 01:44:16 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f8075edcee0>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:44:16 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:44:17 - 0:00:01 - Training..
INFO - 04/12/22 01:46:19 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7fd8d35aaf10>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:46:19 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:46:20 - 0:00:01 - Training..
INFO - 04/12/22 01:48:51 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7ff43d127ca0>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:48:51 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:48:52 - 0:00:01 - Training..
INFO - 04/12/22 01:50:29 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f6c7eceee80>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 01:50:29 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 01:50:30 - 0:00:01 - Training..
INFO - 04/12/22 02:23:08 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 6
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 6
                                     hidden: []
                                     hidden_sz: 768
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 5529, 'Comedy': 3637, 'Romance': 2408, 'Thriller': 1725, 'Horror': 1454, 'Action': 1231, 'Sci-Fi': 1029, 'Adventure': 952, 'Documentary': 862, 'Fantasy': 781, 'Mystery': 701, 'Biography': 423, 'History': 357, 'Western': 348, 'Musical': 322, 'Animation': 215, 'Short': 198, 'Talk-Show': 2})
                                     labels: ['Drama', 'Comedy', 'Romance', 'Documentary', 'Short', 'Action', 'History', 'Adventure', 'Fantasy', 'Sci-Fi', 'Thriller', 'Western', 'Horror', 'Mystery', 'Musical', 'Animation', 'Biography', 'Talk-Show']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 18
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 1
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 300
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 6
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_experiment_in_IMDb/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: mmimdb
                                     task_type: multilabel
                                     train_data_len: 10589
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f23552c9df0>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/12/22 02:23:08 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (x_gate): Linear(in_features=1536, out_features=768, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(300, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(1, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=18, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=3072, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=3072, out_features=768, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_v2a): Linear(in_features=512, out_features=512, bias=True)
                                         (transfm_a2v): Linear(in_features=512, out_features=512, bias=True)
                                       )
                                     )
INFO - 04/12/22 02:23:09 - 0:00:01 - Training..
INFO - 04/12/22 02:52:37 - 0:29:29 - Train Loss: 0.1686
INFO - 04/12/22 02:52:37 - 0:29:29 - Val: Loss: 0.70652
                                     | Micro F1 50.766 | Macro F1: 42.206 | Weighted F1: 58.735 | Samples F1: 52.201 | AP Micro: 55.052
INFO - 04/12/22 03:24:55 - 1:01:47 - Train Loss: 0.1080
INFO - 04/12/22 03:24:55 - 1:01:47 - Val: Loss: 0.66507
                                     | Micro F1 53.023 | Macro F1: 44.877 | Weighted F1: 59.634 | Samples F1: 53.980 | AP Micro: 61.921
INFO - 04/12/22 03:57:55 - 1:34:47 - Train Loss: 0.0805
INFO - 04/12/22 03:57:55 - 1:34:47 - Val: Loss: 0.71974
                                     | Micro F1 60.363 | Macro F1: 52.030 | Weighted F1: 64.010 | Samples F1: 60.491 | AP Micro: 68.711
INFO - 04/12/22 04:31:34 - 2:08:26 - Train Loss: 0.0619
INFO - 04/12/22 04:31:34 - 2:08:26 - Val: Loss: 0.86892
                                     | Micro F1 63.195 | Macro F1: 53.917 | Weighted F1: 65.474 | Samples F1: 64.045 | AP Micro: 71.912
INFO - 04/12/22 05:04:18 - 2:41:10 - Train Loss: 0.0519
INFO - 04/12/22 05:04:18 - 2:41:10 - Val: Loss: 0.80492
                                     | Micro F1 60.974 | Macro F1: 51.910 | Weighted F1: 64.471 | Samples F1: 60.868 | AP Micro: 67.194
INFO - 04/12/22 05:36:29 - 3:13:21 - Train Loss: 0.0442
INFO - 04/12/22 05:36:29 - 3:13:21 - Val: Loss: 0.96067
                                     | Micro F1 64.337 | Macro F1: 54.822 | Weighted F1: 66.536 | Samples F1: 65.236 | AP Micro: 71.094
INFO - 04/12/22 06:08:49 - 3:45:41 - Train Loss: 0.0335
INFO - 04/12/22 06:08:49 - 3:45:41 - Val: Loss: 1.24605
                                     | Micro F1 66.720 | Macro F1: 58.613 | Weighted F1: 67.840 | Samples F1: 67.133 | AP Micro: 73.051
INFO - 04/12/22 06:41:14 - 4:18:06 - Train Loss: 0.0308
INFO - 04/12/22 06:41:14 - 4:18:06 - Val: Loss: 1.19579
                                     | Micro F1 65.742 | Macro F1: 57.291 | Weighted F1: 67.459 | Samples F1: 66.386 | AP Micro: 71.047
INFO - 04/12/22 07:13:03 - 4:49:55 - Train Loss: 0.0256
INFO - 04/12/22 07:13:03 - 4:49:55 - Val: Loss: 1.45320
                                     | Micro F1 66.831 | Macro F1: 57.439 | Weighted F1: 67.527 | Samples F1: 67.410 | AP Micro: 71.951
INFO - 04/12/22 07:45:52 - 5:22:44 - Train Loss: 0.0231
INFO - 04/12/22 07:45:52 - 5:22:44 - Val: Loss: 1.42116
                                     | Micro F1 65.674 | Macro F1: 56.588 | Weighted F1: 66.928 | Samples F1: 66.357 | AP Micro: 71.773
INFO - 04/12/22 08:17:28 - 5:54:20 - Train Loss: 0.0203
INFO - 04/12/22 08:17:28 - 5:54:20 - Val: Loss: 1.39164
                                     | Micro F1 65.690 | Macro F1: 57.005 | Weighted F1: 67.823 | Samples F1: 65.979 | AP Micro: 70.664
INFO - 04/12/22 08:49:30 - 6:26:21 - Train Loss: 0.0170
INFO - 04/12/22 08:49:30 - 6:26:21 - Val: Loss: 1.69120
                                     | Micro F1 67.404 | Macro F1: 58.688 | Weighted F1: 68.152 | Samples F1: 67.713 | AP Micro: 72.971
INFO - 04/12/22 09:21:52 - 6:58:44 - Train Loss: 0.0163
INFO - 04/12/22 09:21:52 - 6:58:44 - Val: Loss: 1.65392
                                     | Micro F1 67.007 | Macro F1: 57.558 | Weighted F1: 67.649 | Samples F1: 67.781 | AP Micro: 72.862
INFO - 04/12/22 09:54:03 - 7:30:55 - Train Loss: 0.0132
INFO - 04/12/22 09:54:03 - 7:30:55 - Val: Loss: 1.85536
                                     | Micro F1 67.996 | Macro F1: 59.210 | Weighted F1: 68.077 | Samples F1: 68.440 | AP Micro: 73.219
INFO - 04/12/22 10:26:37 - 8:03:29 - Train Loss: 0.0116
INFO - 04/12/22 10:26:37 - 8:03:29 - Val: Loss: 1.99013
                                     | Micro F1 67.540 | Macro F1: 58.019 | Weighted F1: 67.547 | Samples F1: 67.903 | AP Micro: 72.910
INFO - 04/12/22 10:57:32 - 8:34:24 - Train Loss: 0.0114
INFO - 04/12/22 10:57:32 - 8:34:24 - Val: Loss: 1.59922
                                     | Micro F1 66.825 | Macro F1: 58.220 | Weighted F1: 67.941 | Samples F1: 67.283 | AP Micro: 71.874
INFO - 04/12/22 11:28:37 - 9:05:29 - Train Loss: 0.0091
INFO - 04/12/22 11:28:37 - 9:05:29 - Val: Loss: 2.13513
                                     | Micro F1 67.909 | Macro F1: 58.624 | Weighted F1: 68.258 | Samples F1: 68.305 | AP Micro: 72.137
INFO - 04/12/22 12:00:23 - 9:37:14 - Train Loss: 0.0045
INFO - 04/12/22 12:00:23 - 9:37:14 - Val: Loss: 2.18479
                                     | Micro F1 68.456 | Macro F1: 59.416 | Weighted F1: 68.578 | Samples F1: 68.772 | AP Micro: 73.899
INFO - 04/12/22 12:33:54 - 10:10:46 - Train Loss: 0.0020
INFO - 04/12/22 12:33:54 - 10:10:46 - Val: Loss: 2.29319
                                      | Micro F1 68.571 | Macro F1: 59.633 | Weighted F1: 68.773 | Samples F1: 68.781 | AP Micro: 73.583
INFO - 04/12/22 13:07:11 - 10:44:03 - Train Loss: 0.0014
INFO - 04/12/22 13:07:11 - 10:44:03 - Val: Loss: 2.48709
                                      | Micro F1 68.343 | Macro F1: 59.633 | Weighted F1: 68.510 | Samples F1: 68.445 | AP Micro: 73.362
INFO - 04/12/22 13:38:47 - 11:15:39 - Train Loss: 0.0009
INFO - 04/12/22 13:38:47 - 11:15:39 - Val: Loss: 2.55461
                                      | Micro F1 69.006 | Macro F1: 60.197 | Weighted F1: 69.026 | Samples F1: 69.079 | AP Micro: 74.026
INFO - 04/12/22 14:10:56 - 11:47:47 - Train Loss: 0.0007
INFO - 04/12/22 14:10:56 - 11:47:47 - Val: Loss: 2.60079
                                      | Micro F1 68.335 | Macro F1: 59.324 | Weighted F1: 68.463 | Samples F1: 68.454 | AP Micro: 73.207
INFO - 04/12/22 14:42:45 - 12:19:37 - Train Loss: 0.0008
INFO - 04/12/22 14:42:45 - 12:19:37 - Val: Loss: 2.62951
                                      | Micro F1 68.318 | Macro F1: 59.475 | Weighted F1: 68.464 | Samples F1: 68.588 | AP Micro: 73.244
INFO - 04/12/22 15:14:01 - 12:50:52 - Train Loss: 0.0009
INFO - 04/12/22 15:14:01 - 12:50:52 - Val: Loss: 2.62195
                                      | Micro F1 68.711 | Macro F1: 60.354 | Weighted F1: 68.984 | Samples F1: 68.718 | AP Micro: 72.809
INFO - 04/12/22 15:45:40 - 13:22:32 - Train Loss: 0.0005
INFO - 04/12/22 15:45:40 - 13:22:32 - Val: Loss: 2.77675
                                      | Micro F1 69.071 | Macro F1: 60.666 | Weighted F1: 69.071 | Samples F1: 69.196 | AP Micro: 73.390
INFO - 04/12/22 16:18:29 - 13:55:21 - Train Loss: 0.0003
INFO - 04/12/22 16:18:29 - 13:55:21 - Val: Loss: 2.73961
                                      | Micro F1 69.210 | Macro F1: 60.301 | Weighted F1: 69.258 | Samples F1: 69.296 | AP Micro: 73.102
INFO - 04/12/22 16:51:19 - 14:28:10 - Train Loss: 0.0002
INFO - 04/12/22 16:51:19 - 14:28:10 - Val: Loss: 2.72967
                                      | Micro F1 69.048 | Macro F1: 60.644 | Weighted F1: 69.246 | Samples F1: 69.171 | AP Micro: 73.147
INFO - 04/12/22 17:23:10 - 15:00:02 - Train Loss: 0.0002
INFO - 04/12/22 17:23:10 - 15:00:02 - Val: Loss: 2.93062
                                      | Micro F1 69.077 | Macro F1: 60.401 | Weighted F1: 69.052 | Samples F1: 69.006 | AP Micro: 73.315
INFO - 04/12/22 17:54:12 - 15:31:03 - Train Loss: 0.0003
INFO - 04/12/22 17:54:12 - 15:31:03 - Val: Loss: 2.80805
                                      | Micro F1 68.733 | Macro F1: 60.543 | Weighted F1: 68.883 | Samples F1: 68.848 | AP Micro: 72.880
INFO - 04/12/22 18:25:35 - 16:02:27 - Train Loss: 0.0002
INFO - 04/12/22 18:25:35 - 16:02:27 - Val: Loss: 2.91569
                                      | Micro F1 68.798 | Macro F1: 59.965 | Weighted F1: 68.825 | Samples F1: 68.820 | AP Micro: 72.946
INFO - 04/12/22 18:56:53 - 16:33:45 - Train Loss: 0.0002
INFO - 04/12/22 18:56:53 - 16:33:45 - Val: Loss: 2.98623
                                      | Micro F1 69.036 | Macro F1: 60.642 | Weighted F1: 69.062 | Samples F1: 69.019 | AP Micro: 73.150
INFO - 04/12/22 19:29:10 - 17:06:02 - Train Loss: 0.0001
INFO - 04/12/22 19:29:10 - 17:06:02 - Val: Loss: 3.00173
                                      | Micro F1 68.995 | Macro F1: 60.637 | Weighted F1: 68.997 | Samples F1: 69.065 | AP Micro: 73.362
INFO - 04/12/22 19:29:46 - 17:06:38 - No improvement. Breaking out of loop.
DEBUG - 04/12/22 19:29:46 - 17:06:38 - Starting new HTTPS connection (1): s3.amazonaws.com:443
INFO - 04/12/22 19:29:51 - 17:06:43 - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/est_posgrado_diego.moreno/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
DEBUG - 04/12/22 19:29:51 - 17:06:43 - Starting new HTTPS connection (1): s3.amazonaws.com:443
INFO - 04/12/22 19:29:56 - 17:06:48 - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/est_posgrado_diego.moreno/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
DEBUG - 04/12/22 19:29:56 - 17:06:48 - Starting new HTTPS connection (1): huggingface.co:443
DEBUG - 04/12/22 19:30:01 - 17:06:53 - Starting new HTTPS connection (1): huggingface.co:443
