INFO - 09/08/21 23:34:30 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 8
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 16
                                     hidden: []
                                     hidden_sz: 768
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 1803, 'Comedy': 1276, 'Thriller': 1010, 'Action': 825, 'Romance': 767, 'Fantasy': 665, 'Crime': 610, 'Sci-Fi': 424, 'Horror': 395, 'Family': 385, 'Mystery': 343, 'Biography': 207, 'Animation': 175})
                                     labels: ['Mystery', 'Thriller', 'Comedy', 'Action', 'Crime', 'Drama', 'Fantasy', 'Family', 'Horror', 'Biography', 'Romance', 'Sci-Fi', 'Animation']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 13
                                     n_workers: 12
                                     name: moviescope_Seed5_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 6
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 96
                                     orig_d_l: 768
                                     orig_d_m: 312
                                     orig_d_v: 4096
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 5
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/model_save_mmtr_hybrid_v4/moviescope_Seed1_mmbt_model_run/moviescope_Seed2_mmbt_model_run/moviescope_Seed3_mmbt_model_run/moviescope_Seed4_mmbt_model_run/moviescope_Seed5_mmbt_model_run
                                     seed: 5
                                     task: moviescope
                                     task_type: multilabel
                                     train_data_len: 3449
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f92f7e26910>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 09/08/21 23:34:30 - 0:00:00 - DataParallel(
                                       (module): MMTransformerGMUHybridClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (audio_enc): AudioEncoder(
                                           (conv_layers): ModuleList(
                                             (0): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (1): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (2): AdaptiveAvgPool1d(output_size=200)
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (proj2_poster): Linear(in_features=4096, out_features=768, bias=False)
                                         (proj_l_e): Linear(in_features=768, out_features=768, bias=False)
                                         (proj_v_e): Linear(in_features=4096, out_features=768, bias=False)
                                         (proj_a_e): Linear(in_features=96, out_features=768, bias=False)
                                         (proj_l): Conv1d(768, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(4096, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(96, 768, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=768, out_features=768, bias=True)
                                               )
                                               (fc1): Linear(in_features=768, out_features=3072, bias=True)
                                               (fc2): Linear(in_features=3072, out_features=768, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_mem): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_mem): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_mem): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=1536, out_features=1536, bias=True)
                                               )
                                               (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                                               (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=768, out_features=768, bias=True)
                                         (proj2): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer): Linear(in_features=768, out_features=13, bias=True)
                                         (proj12): Linear(in_features=768, out_features=768, bias=True)
                                         (proj22): Linear(in_features=768, out_features=768, bias=True)
                                         (out_layer2): Linear(in_features=768, out_features=13, bias=True)
                                         (out_layer_final): Linear(in_features=13, out_features=13, bias=True)
                                         (gmu): TextShifting5Layer(
                                           (hidden1): Linear(in_features=1536, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=1536, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=1536, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden5): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=6144, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=6144, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=6144, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=6144, out_features=768, bias=False)
                                           (x5_gate): Linear(in_features=6144, out_features=768, bias=False)
                                         )
                                         (gmuSimple): TextShifting4LayerSimple(
                                           (hidden1): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden2): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden3): Linear(in_features=768, out_features=768, bias=False)
                                           (hidden4): Linear(in_features=768, out_features=768, bias=False)
                                           (x1_gate): Linear(in_features=768, out_features=768, bias=False)
                                           (x2_gate): Linear(in_features=768, out_features=768, bias=False)
                                           (x3_gate): Linear(in_features=768, out_features=768, bias=False)
                                           (x4_gate): Linear(in_features=768, out_features=768, bias=False)
                                         )
                                       )
                                     )
INFO - 09/08/21 23:34:31 - 0:00:01 - Training..
INFO - 09/08/21 23:50:15 - 0:15:45 - Train Loss: 0.0719
INFO - 09/08/21 23:50:15 - 0:15:45 - Val: Loss: 1.05562 | Macro F1 0.43228 | Micro F1: 0.46206 | AUC PR Macro: 0.43204 | AUC PR Micro: 0.51551
INFO - 09/09/21 00:07:44 - 0:33:14 - Train Loss: 0.0602
INFO - 09/09/21 00:07:44 - 0:33:14 - Val: Loss: 0.99112 | Macro F1 0.46192 | Micro F1: 0.50160 | AUC PR Macro: 0.46181 | AUC PR Micro: 0.50239
INFO - 09/09/21 00:24:35 - 0:50:04 - Train Loss: 0.0537
INFO - 09/09/21 00:24:35 - 0:50:04 - Val: Loss: 0.98117 | Macro F1 0.48834 | Micro F1: 0.51852 | AUC PR Macro: 0.49119 | AUC PR Micro: 0.52548
INFO - 09/09/21 00:42:28 - 1:07:58 - Train Loss: 0.0476
INFO - 09/09/21 00:42:28 - 1:07:58 - Val: Loss: 0.93753 | Macro F1 0.49835 | Micro F1: 0.55380 | AUC PR Macro: 0.52993 | AUC PR Micro: 0.56680
INFO - 09/09/21 01:00:21 - 1:25:51 - Train Loss: 0.0415
INFO - 09/09/21 01:00:21 - 1:25:51 - Val: Loss: 0.92568 | Macro F1 0.53927 | Micro F1: 0.58767 | AUC PR Macro: 0.56181 | AUC PR Micro: 0.61923
INFO - 09/09/21 01:17:52 - 1:43:22 - Train Loss: 0.0359
INFO - 09/09/21 01:17:52 - 1:43:22 - Val: Loss: 0.90956 | Macro F1 0.55343 | Micro F1: 0.60928 | AUC PR Macro: 0.59157 | AUC PR Micro: 0.65442
INFO - 09/09/21 01:35:50 - 2:01:20 - Train Loss: 0.0310
INFO - 09/09/21 01:35:50 - 2:01:20 - Val: Loss: 0.88843 | Macro F1 0.57075 | Micro F1: 0.61395 | AUC PR Macro: 0.61288 | AUC PR Micro: 0.66416
INFO - 09/09/21 01:54:17 - 2:19:47 - Train Loss: 0.0270
INFO - 09/09/21 01:54:17 - 2:19:47 - Val: Loss: 0.90960 | Macro F1 0.60196 | Micro F1: 0.63748 | AUC PR Macro: 0.63607 | AUC PR Micro: 0.68135
INFO - 09/09/21 02:11:42 - 2:37:12 - Train Loss: 0.0233
INFO - 09/09/21 02:11:42 - 2:37:12 - Val: Loss: 0.98207 | Macro F1 0.60266 | Micro F1: 0.64683 | AUC PR Macro: 0.63443 | AUC PR Micro: 0.67435
INFO - 09/09/21 02:28:20 - 2:53:50 - Train Loss: 0.0203
INFO - 09/09/21 02:28:20 - 2:53:50 - Val: Loss: 1.02605 | Macro F1 0.61457 | Micro F1: 0.65027 | AUC PR Macro: 0.64412 | AUC PR Micro: 0.67852
INFO - 09/09/21 02:45:56 - 3:11:26 - Train Loss: 0.0170
INFO - 09/09/21 02:45:56 - 3:11:26 - Val: Loss: 1.09673 | Macro F1 0.61609 | Micro F1: 0.65462 | AUC PR Macro: 0.66067 | AUC PR Micro: 0.70615
INFO - 09/09/21 03:03:13 - 3:28:43 - Train Loss: 0.0148
INFO - 09/09/21 03:03:13 - 3:28:43 - Val: Loss: 1.22734 | Macro F1 0.61523 | Micro F1: 0.66619 | AUC PR Macro: 0.65878 | AUC PR Micro: 0.71827
INFO - 09/09/21 03:20:46 - 3:46:16 - Train Loss: 0.0118
INFO - 09/09/21 03:20:46 - 3:46:16 - Val: Loss: 1.32651 | Macro F1 0.60990 | Micro F1: 0.66790 | AUC PR Macro: 0.65279 | AUC PR Micro: 0.71683
INFO - 09/09/21 03:37:59 - 4:03:29 - Train Loss: 0.0103
INFO - 09/09/21 03:37:59 - 4:03:29 - Val: Loss: 1.34608 | Macro F1 0.61937 | Micro F1: 0.66987 | AUC PR Macro: 0.65671 | AUC PR Micro: 0.72290
INFO - 09/09/21 03:55:30 - 4:21:00 - Train Loss: 0.0085
INFO - 09/09/21 03:55:30 - 4:21:00 - Val: Loss: 1.49225 | Macro F1 0.61185 | Micro F1: 0.67271 | AUC PR Macro: 0.66229 | AUC PR Micro: 0.71894
INFO - 09/09/21 04:12:19 - 4:37:49 - Train Loss: 0.0070
INFO - 09/09/21 04:12:19 - 4:37:49 - Val: Loss: 1.42818 | Macro F1 0.61864 | Micro F1: 0.66667 | AUC PR Macro: 0.66159 | AUC PR Micro: 0.70872
INFO - 09/09/21 04:28:37 - 4:54:07 - Train Loss: 0.0055
INFO - 09/09/21 04:28:37 - 4:54:07 - Val: Loss: 1.55836 | Macro F1 0.62289 | Micro F1: 0.66921 | AUC PR Macro: 0.66878 | AUC PR Micro: 0.72671
INFO - 09/09/21 04:46:09 - 5:11:39 - Train Loss: 0.0046
INFO - 09/09/21 04:46:09 - 5:11:39 - Val: Loss: 1.69861 | Macro F1 0.61851 | Micro F1: 0.66564 | AUC PR Macro: 0.66967 | AUC PR Micro: 0.72468
INFO - 09/09/21 05:02:27 - 5:27:56 - Train Loss: 0.0036
INFO - 09/09/21 05:02:27 - 5:27:56 - Val: Loss: 1.67985 | Macro F1 0.61201 | Micro F1: 0.66389 | AUC PR Macro: 0.66157 | AUC PR Micro: 0.72087
INFO - 09/09/21 05:19:08 - 5:44:38 - Train Loss: 0.0029
INFO - 09/09/21 05:19:08 - 5:44:38 - Val: Loss: 1.85788 | Macro F1 0.61550 | Micro F1: 0.66824 | AUC PR Macro: 0.66693 | AUC PR Micro: 0.72427
INFO - 09/09/21 05:35:36 - 6:01:06 - Train Loss: 0.0023
INFO - 09/09/21 05:35:36 - 6:01:06 - Val: Loss: 1.84901 | Macro F1 0.62507 | Micro F1: 0.67257 | AUC PR Macro: 0.67178 | AUC PR Micro: 0.72915
INFO - 09/09/21 05:52:14 - 6:17:44 - Train Loss: 0.0021
INFO - 09/09/21 05:52:14 - 6:17:44 - Val: Loss: 1.89294 | Macro F1 0.62426 | Micro F1: 0.67441 | AUC PR Macro: 0.67008 | AUC PR Micro: 0.72433
INFO - 09/09/21 06:08:33 - 6:34:03 - Train Loss: 0.0018
INFO - 09/09/21 06:08:33 - 6:34:03 - Val: Loss: 1.96705 | Macro F1 0.61838 | Micro F1: 0.67188 | AUC PR Macro: 0.67497 | AUC PR Micro: 0.72965
INFO - 09/09/21 06:25:50 - 6:51:20 - Train Loss: 0.0016
INFO - 09/09/21 06:25:50 - 6:51:20 - Val: Loss: 1.99786 | Macro F1 0.61826 | Micro F1: 0.67034 | AUC PR Macro: 0.67080 | AUC PR Micro: 0.72740
INFO - 09/09/21 06:42:04 - 7:07:34 - Train Loss: 0.0014
INFO - 09/09/21 06:42:04 - 7:07:34 - Val: Loss: 2.05290 | Macro F1 0.62253 | Micro F1: 0.67165 | AUC PR Macro: 0.66972 | AUC PR Micro: 0.72724
INFO - 09/09/21 06:58:40 - 7:24:10 - Train Loss: 0.0012
INFO - 09/09/21 06:58:40 - 7:24:10 - Val: Loss: 2.05782 | Macro F1 0.61708 | Micro F1: 0.66772 | AUC PR Macro: 0.66978 | AUC PR Micro: 0.72590
INFO - 09/09/21 07:14:56 - 7:40:26 - Train Loss: 0.0011
INFO - 09/09/21 07:14:56 - 7:40:26 - Val: Loss: 2.07726 | Macro F1 0.62301 | Micro F1: 0.67060 | AUC PR Macro: 0.67390 | AUC PR Micro: 0.72836
INFO - 09/09/21 07:31:16 - 7:56:46 - Train Loss: 0.0010
INFO - 09/09/21 07:31:16 - 7:56:46 - Val: Loss: 2.11226 | Macro F1 0.62032 | Micro F1: 0.67168 | AUC PR Macro: 0.67399 | AUC PR Micro: 0.72911
INFO - 09/09/21 07:31:55 - 7:57:25 - No improvement. Breaking out of loop.
INFO - 09/09/21 07:38:20 - 8:03:50 - Test - : Loss: 1.92954 | Macro F1 0.63846 | Micro F1: 0.68214 | AUC PR Macro: 0.69315 | AUC PR Micro: 0.74298
