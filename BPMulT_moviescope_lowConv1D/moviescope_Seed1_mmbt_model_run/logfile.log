INFO - 04/27/22 01:43:37 - 0:00:00 - a_len: 3
                                     adapter_size: 64
                                     aonly: True
                                     attn_dropout: 0.1
                                     attn_dropout_a: 0.0
                                     attn_dropout_v: 0.0
                                     attn_mask: True
                                     batch_sz: 8
                                     bert_model: bert-base-uncased
                                     chunk_size: 100
                                     config_file: /home/est_posgrado_diego.moreno/vilbert-multi-task/config/bert_base_6layer_6conect.json
                                     data_path: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/datasets
                                     drop_img_percent: 0.0
                                     dropout: 0.1
                                     embed_dropout: 0.25
                                     embed_sz: 300
                                     freeze_img: 0
                                     freeze_txt: 0
                                     from_pretrained: /home/est_posgrado_diego.moreno/vilbert-multi-task/save/multi_task_model.bin
                                     from_seed: 1
                                     glove_path: /path/to/glove_embeds/glove.840B.300d.txt
                                     gradient_accumulation_steps: 8
                                     hidden: []
                                     hidden_sz: 500
                                     hybrid: False
                                     img_embed_pool_type: avg
                                     img_hidden_sz: 2048
                                     include_bn: True
                                     inverse_seed: False
                                     just_test: False
                                     l_len: 512
                                     label_freqs: Counter({'Drama': 1803, 'Comedy': 1276, 'Thriller': 1010, 'Action': 825, 'Romance': 767, 'Fantasy': 665, 'Crime': 610, 'Sci-Fi': 424, 'Horror': 395, 'Family': 385, 'Mystery': 343, 'Biography': 207, 'Animation': 175})
                                     labels: ['Mystery', 'Thriller', 'Comedy', 'Action', 'Crime', 'Drama', 'Fantasy', 'Family', 'Horror', 'Biography', 'Romance', 'Sci-Fi', 'Animation']
                                     layers: 5
                                     lonly: True
                                     lr: 5e-05
                                     lr_factor: 0.5
                                     lr_patience: 2
                                     max_epochs: 100
                                     max_seq_len: 512
                                     model: mmtrvapt
                                     n_classes: 13
                                     n_workers: 12
                                     name: moviescope_Seed1_mmbt_model_run
                                     nlevels: 5
                                     num_heads: 10
                                     num_image_embeds: 3
                                     num_images: 8
                                     orig_d_a: 96
                                     orig_d_l: 768
                                     orig_d_p: 4096
                                     orig_d_v: 4096
                                     out_dropout: 0.0
                                     output_gates: False
                                     patience: 8
                                     pooling: cls
                                     relu_dropout: 0.1
                                     res_dropout: 0.1
                                     savedir: /home/est_posgrado_diego.moreno/Workspace/Tesis/multimodal-transformers-movies/BPMulT_moviescope_lowConv1D/moviescope_Seed1_mmbt_model_run
                                     seed: 1
                                     task: moviescope
                                     task_type: multilabel
                                     train_data_len: 3449
                                     train_type: split
                                     trained_model_dir: 
                                     v_len: 3
                                     vision_scratch: False
                                     visual: both
                                     vocab: <mmbt.data.vocab.Vocab object at 0x7f4a8d6bfd60>
                                     vocab_sz: 30522
                                     vonly: True
                                     warmup: 0.1
                                     weight_classes: 1
INFO - 04/27/22 01:43:37 - 0:00:00 - DataParallel(
                                       (module): MultiprojectionMMTransformerGMUClf(
                                         (enc): BertEncoder(
                                           (bert): BertModel(
                                             (embeddings): BertEmbeddings(
                                               (word_embeddings): Embedding(30522, 768, padding_idx=0)
                                               (position_embeddings): Embedding(512, 768)
                                               (token_type_embeddings): Embedding(2, 768)
                                               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                               (dropout): Dropout(p=0.1, inplace=False)
                                             )
                                             (encoder): BertEncoder(
                                               (layer): ModuleList(
                                                 (0): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (1): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (2): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (3): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (4): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (5): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (6): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (7): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (8): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (9): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (10): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                                 (11): BertLayer(
                                                   (attention): BertAttention(
                                                     (self): BertSelfAttention(
                                                       (query): Linear(in_features=768, out_features=768, bias=True)
                                                       (key): Linear(in_features=768, out_features=768, bias=True)
                                                       (value): Linear(in_features=768, out_features=768, bias=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                     )
                                                     (output): BertSelfOutput(
                                                       (dense): Linear(in_features=768, out_features=768, bias=True)
                                                       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                       (dropout): Dropout(p=0.1, inplace=False)
                                                       (attention_text_task_adapters): ModuleDict()
                                                       (adapter_fusion_layer): ModuleDict()
                                                       (attention_text_lang_adapters): ModuleDict()
                                                     )
                                                   )
                                                   (intermediate): BertIntermediate(
                                                     (dense): Linear(in_features=768, out_features=3072, bias=True)
                                                   )
                                                   (output): BertOutput(
                                                     (dense): Linear(in_features=3072, out_features=768, bias=True)
                                                     (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                                                     (dropout): Dropout(p=0.1, inplace=False)
                                                     (adapter_fusion_layer): ModuleDict()
                                                     (layer_text_task_adapters): ModuleDict()
                                                     (layer_text_lang_adapters): ModuleDict()
                                                   )
                                                 )
                                               )
                                             )
                                             (pooler): BertPooler(
                                               (dense): Linear(in_features=768, out_features=768, bias=True)
                                               (activation): Tanh()
                                             )
                                             (invertible_lang_adapters): ModuleDict()
                                           )
                                         )
                                         (audio_enc): AudioEncoder(
                                           (conv_layers): ModuleList(
                                             (0): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (1): Conv1d(96, 96, kernel_size=(128,), stride=(2,))
                                             (2): AdaptiveAvgPool1d(output_size=200)
                                           )
                                         )
                                         (proj_poster): Linear(in_features=4096, out_features=500, bias=False)
                                         (gmu_l_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (gmu_v_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (gmu_a_m): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (gmu_l): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (gmu_v): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (gmu_a): GatedMultimodalLayerFeatures(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (x_gate): Linear(in_features=1000, out_features=500, bias=False)
                                         )
                                         (proj_l): Conv1d(768, 500, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_v): Conv1d(4096, 500, kernel_size=(1,), stride=(1,), bias=False)
                                         (proj_a): Conv1d(96, 500, kernel_size=(1,), stride=(1,), bias=False)
                                         (trans_l_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_v2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_l_with_a2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_l2a): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_v_with_a2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_v2l): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (trans_a_with_l2v): TransformerEncoder(
                                           (embed_positions): SinusoidalPositionalEmbedding()
                                           (layers): ModuleList(
                                             (0): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (1): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (2): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (3): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                             (4): TransformerEncoderLayer(
                                               (self_attn): MultiheadAttention(
                                                 (out_proj): Linear(in_features=500, out_features=500, bias=True)
                                               )
                                               (fc1): Linear(in_features=500, out_features=2000, bias=True)
                                               (fc2): Linear(in_features=2000, out_features=500, bias=True)
                                               (layer_norms): ModuleList(
                                                 (0): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                                 (1): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                               )
                                             )
                                           )
                                           (layer_norm): LayerNorm((500,), eps=1e-05, elementwise_affine=True)
                                         )
                                         (proj1): Linear(in_features=500, out_features=500, bias=True)
                                         (proj2): Linear(in_features=500, out_features=500, bias=True)
                                         (out_layer): Linear(in_features=500, out_features=13, bias=True)
                                         (gmu): TextShifting4Layer(
                                           (hidden1): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden2): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden3): Linear(in_features=500, out_features=500, bias=False)
                                           (hidden4): Linear(in_features=500, out_features=500, bias=False)
                                           (x1_gate): Linear(in_features=2000, out_features=500, bias=False)
                                           (x2_gate): Linear(in_features=2000, out_features=500, bias=False)
                                           (x3_gate): Linear(in_features=2000, out_features=500, bias=False)
                                           (x4_gate): Linear(in_features=2000, out_features=500, bias=False)
                                         )
                                         (transfm_a2l): Linear(in_features=200, out_features=512, bias=True)
                                         (transfm_v2l): Linear(in_features=200, out_features=512, bias=True)
                                         (transfm_l2a): Linear(in_features=512, out_features=200, bias=True)
                                         (transfm_l2v): Linear(in_features=512, out_features=200, bias=True)
                                       )
                                     )
INFO - 04/27/22 01:43:38 - 0:00:01 - Training..
INFO - 04/27/22 01:51:20 - 0:07:42 - Train Loss: 0.1210
INFO - 04/27/22 01:51:20 - 0:07:42 - Val: Loss: 0.81413
                                     | Macro F1 54.658 | Micro F1: 57.311 | AP Macro: 66.233 | AP Micro: 58.925 | AP Samples: 71.910
INFO - 04/27/22 02:00:15 - 0:16:38 - Train Loss: 0.0809
INFO - 04/27/22 02:00:15 - 0:16:38 - Val: Loss: 0.70495
                                     | Macro F1 65.065 | Micro F1: 67.692 | AP Macro: 72.537 | AP Micro: 74.026 | AP Samples: 83.111
INFO - 04/27/22 02:09:16 - 0:25:39 - Train Loss: 0.0610
INFO - 04/27/22 02:09:16 - 0:25:39 - Val: Loss: 0.72418
                                     | Macro F1 63.890 | Micro F1: 67.631 | AP Macro: 74.431 | AP Micro: 74.436 | AP Samples: 82.836
INFO - 04/27/22 02:17:54 - 0:34:17 - Train Loss: 0.0445
INFO - 04/27/22 02:17:54 - 0:34:17 - Val: Loss: 0.79208
                                     | Macro F1 66.063 | Micro F1: 70.152 | AP Macro: 75.302 | AP Micro: 77.110 | AP Samples: 84.191
INFO - 04/27/22 02:26:53 - 0:43:16 - Train Loss: 0.0327
INFO - 04/27/22 02:26:53 - 0:43:16 - Val: Loss: 0.88836
                                     | Macro F1 69.723 | Micro F1: 71.952 | AP Macro: 76.284 | AP Micro: 78.664 | AP Samples: 84.603
INFO - 04/27/22 02:35:40 - 0:52:03 - Train Loss: 0.0240
INFO - 04/27/22 02:35:40 - 0:52:03 - Val: Loss: 0.93060
                                     | Macro F1 68.599 | Micro F1: 70.968 | AP Macro: 74.824 | AP Micro: 77.493 | AP Samples: 83.801
INFO - 04/27/22 02:44:15 - 1:00:38 - Train Loss: 0.0178
INFO - 04/27/22 02:44:15 - 1:00:38 - Val: Loss: 1.12351
                                     | Macro F1 68.687 | Micro F1: 71.666 | AP Macro: 75.737 | AP Micro: 79.188 | AP Samples: 85.785
INFO - 04/27/22 02:52:48 - 1:09:11 - Train Loss: 0.0133
INFO - 04/27/22 02:52:48 - 1:09:11 - Val: Loss: 1.10685
                                     | Macro F1 68.566 | Micro F1: 71.947 | AP Macro: 75.220 | AP Micro: 79.098 | AP Samples: 85.524
INFO - 04/27/22 03:01:16 - 1:17:39 - Train Loss: 0.0091
INFO - 04/27/22 03:01:16 - 1:17:39 - Val: Loss: 1.25038
                                     | Macro F1 68.888 | Micro F1: 72.239 | AP Macro: 75.520 | AP Micro: 78.443 | AP Samples: 84.698
INFO - 04/27/22 03:09:51 - 1:26:14 - Train Loss: 0.0064
INFO - 04/27/22 03:09:51 - 1:26:14 - Val: Loss: 1.31890
                                     | Macro F1 70.285 | Micro F1: 73.258 | AP Macro: 76.306 | AP Micro: 79.692 | AP Samples: 85.030
INFO - 04/27/22 03:18:45 - 1:35:08 - Train Loss: 0.0049
INFO - 04/27/22 03:18:45 - 1:35:08 - Val: Loss: 1.41030
                                     | Macro F1 69.262 | Micro F1: 73.118 | AP Macro: 76.209 | AP Micro: 79.905 | AP Samples: 86.124
INFO - 04/27/22 03:27:35 - 1:43:58 - Train Loss: 0.0029
INFO - 04/27/22 03:27:35 - 1:43:58 - Val: Loss: 1.40344
                                     | Macro F1 70.449 | Micro F1: 73.163 | AP Macro: 76.382 | AP Micro: 79.559 | AP Samples: 85.401
INFO - 04/27/22 03:35:57 - 1:52:20 - Train Loss: 0.0023
INFO - 04/27/22 03:35:57 - 1:52:20 - Val: Loss: 1.52659
                                     | Macro F1 69.824 | Micro F1: 73.163 | AP Macro: 75.926 | AP Micro: 79.285 | AP Samples: 85.151
INFO - 04/27/22 03:44:29 - 2:00:51 - Train Loss: 0.0015
INFO - 04/27/22 03:44:29 - 2:00:51 - Val: Loss: 1.54194
                                     | Macro F1 70.032 | Micro F1: 73.084 | AP Macro: 75.586 | AP Micro: 79.250 | AP Samples: 84.873
INFO - 04/27/22 03:52:50 - 2:09:13 - Train Loss: 0.0009
INFO - 04/27/22 03:52:50 - 2:09:13 - Val: Loss: 1.66375
                                     | Macro F1 70.198 | Micro F1: 73.758 | AP Macro: 75.915 | AP Micro: 79.643 | AP Samples: 85.345
INFO - 04/27/22 04:01:33 - 2:17:56 - Train Loss: 0.0007
INFO - 04/27/22 04:01:33 - 2:17:56 - Val: Loss: 1.66788
                                     | Macro F1 69.905 | Micro F1: 73.551 | AP Macro: 76.156 | AP Micro: 80.003 | AP Samples: 85.522
INFO - 04/27/22 04:10:12 - 2:26:35 - Train Loss: 0.0005
INFO - 04/27/22 04:10:12 - 2:26:35 - Val: Loss: 1.72304
                                     | Macro F1 69.856 | Micro F1: 73.561 | AP Macro: 76.232 | AP Micro: 79.678 | AP Samples: 85.549
INFO - 04/27/22 04:18:35 - 2:34:58 - Train Loss: 0.0005
INFO - 04/27/22 04:18:35 - 2:34:58 - Val: Loss: 1.73897
                                     | Macro F1 70.258 | Micro F1: 73.937 | AP Macro: 76.286 | AP Micro: 79.744 | AP Samples: 85.530
INFO - 04/27/22 04:27:07 - 2:43:30 - Train Loss: 0.0004
INFO - 04/27/22 04:27:07 - 2:43:30 - Val: Loss: 1.72543
                                     | Macro F1 70.561 | Micro F1: 74.019 | AP Macro: 76.277 | AP Micro: 79.641 | AP Samples: 85.461
INFO - 04/27/22 04:35:32 - 2:51:54 - Train Loss: 0.0004
INFO - 04/27/22 04:35:32 - 2:51:54 - Val: Loss: 1.76655
                                     | Macro F1 69.535 | Micro F1: 73.299 | AP Macro: 76.265 | AP Micro: 79.742 | AP Samples: 85.581
INFO - 04/27/22 04:44:04 - 3:00:27 - Train Loss: 0.0003
INFO - 04/27/22 04:44:04 - 3:00:27 - Val: Loss: 1.76780
                                     | Macro F1 69.944 | Micro F1: 73.593 | AP Macro: 76.301 | AP Micro: 79.796 | AP Samples: 85.672
INFO - 04/27/22 04:52:21 - 3:08:43 - Train Loss: 0.0003
INFO - 04/27/22 04:52:21 - 3:08:43 - Val: Loss: 1.78163
                                     | Macro F1 70.152 | Micro F1: 73.664 | AP Macro: 76.273 | AP Micro: 79.767 | AP Samples: 85.568
INFO - 04/27/22 05:00:45 - 3:17:08 - Train Loss: 0.0003
INFO - 04/27/22 05:00:45 - 3:17:08 - Val: Loss: 1.79326
                                     | Macro F1 69.964 | Micro F1: 73.407 | AP Macro: 76.271 | AP Micro: 79.758 | AP Samples: 85.490
INFO - 04/27/22 05:09:18 - 3:25:40 - Train Loss: 0.0003
INFO - 04/27/22 05:09:18 - 3:25:40 - Val: Loss: 1.78903
                                     | Macro F1 70.104 | Micro F1: 73.569 | AP Macro: 76.268 | AP Micro: 79.749 | AP Samples: 85.614
INFO - 04/27/22 05:09:34 - 3:25:56 - No improvement. Breaking out of loop.
DEBUG - 04/27/22 05:09:34 - 3:25:57 - Starting new HTTPS connection (1): s3.amazonaws.com:443
INFO - 04/27/22 05:09:39 - 3:26:02 - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/est_posgrado_diego.moreno/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
DEBUG - 04/27/22 05:09:39 - 3:26:02 - Starting new HTTPS connection (1): s3.amazonaws.com:443
INFO - 04/27/22 05:09:44 - 3:26:07 - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/est_posgrado_diego.moreno/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
DEBUG - 04/27/22 05:09:45 - 3:26:08 - Starting new HTTPS connection (1): huggingface.co:443
DEBUG - 04/27/22 05:09:55 - 3:26:18 - Starting new HTTPS connection (1): huggingface.co:443
